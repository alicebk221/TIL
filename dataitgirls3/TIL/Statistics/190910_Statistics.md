
# 지난 시간 복습
* 실험에는 실험자의 개입이 들어가야만 한다. -> 날씨 등에는 개입할 수 없기 때문에 실험이라고 할 수 없다.
* 이미 그 상태인 사람들을 모으는 것도 행동에 대한 개입이 아니기 때문에 실험이 아님.
* 두 집단의 평균 차이를 검정하기 위해 부트스트래핑 할 때는 두 집단이 원래 같다고 가정한다.
* 두 집단의 평균 차이를 검정하기 위해 부트스트래핑 할 때는 집단을 무시하고 데이터를 섞은 후, 2개의 샘플을 리샘플링으로 뽑는다.
* 1종 오류(=False Alarm) : 실제로는 일어나지 않았는데 일어났다고 나오는 것
* 2종 오류(=Miss) : 실제로는 일어났는데 일어나지 않았다고 나오는 것
* 부트스트래핑에서 평균 차이의 95% 신뢰구간을 구했다면 신뢰수준은 95%이다.
* 1종 오류의 대표적인 예시 : 재현성 위기 (ex. power pose)
* 귀무 가설(Null Hypothesis) : 같다고 가정하는 것. Nullified되는 가설. 결론은 다르다고 내는 것이 목적이기 때문에 사라질 가설. 따라서 귀무 가설은 기각하는 것이 목적이기 때문에 채택한다는 표현은 거의 사용하지 않는다.


# 효과 크기 (Effect Size)
* 표현 : 분산의 n%를 설명한다.

## 에타 제곱


```python
import pandas as pd
import numpy
sleep = pd.read_csv('sleep.csv')
```


```python
전체평균 = sleep['extra'].mean() # extra의 전체 평균
X = sleep['extra']
전체SS = numpy.sum((X - 전체평균) ** 2)

# sleep을 group 변수에 따라 그룹을 지어(groupby)
# extra 컬럼을 평균(mean)내는 작업
sleep.groupby('group').agg({'extra': 'mean'})   # agg : aggregate, 모아서 통계낼 때 사용
```


```python
# 개인의 특성은 무시하고 집단 간의 차이만 가정
# 처치SS는 그룹SS 등으로 쓸 수도 있다
처치SS = (
    10 * (0.75 - 전체평균) ** 2 +      # 1번 그룹의 10명의 사람
    10 * (2.33 - 전체평균) ** 2)       # 2번 그룹의 10명의 사람

에타제곱 = 처치SS / 전체SS
에타제곱                    # 집단 간의 차이가 수면시간 (분산)의 16.1%를 설명한다.
```

* 개인차 = 집단 간의 차이 + 집단 내의 차이
* 에타제곱은 전체 차이를 집단 간의 차이로 나눈 것. 즉 전체 차이에서 집단 간의 차이가 차지하는 비중을 알아보는 것.


```python
# 두 집단의 표준편차 차이가 0.78 (ex. IQ 11점 정도의 차이, IQ 테스트에서 표준편차 1 차이 = 15점)
코헨의d = (2.33-0.75) / sleep['extra'].std()
```

# 설문 만들기
* 한 번에 한 가지만 물어본다
* 응답을 수치화하기 쉬운 형태로 질문한다

## 리커트 척도
* 그렇다/아니다를 단계로 나누어 질문
    * ex. 전혀 그렇지 않다(1)-약간 그렇지 않다(2)-보통이다(3)-약간 그렇다(4)-매우 그렇다(5)

## 거트만 척도
* 응답자가 응답하기에 편하다

## 강제 선택형
* 객관식 시험처럼 정답이 있는 경우가 아니면 통계 처리가 까다롭다

## 진점수 이론 (The Score Theory)
* X(설문) = T(진짜 고객 만족도) + E(오차)
* E로 인해 발생하는 문제를 최소화하기 위해, T는 그대로 두고 E를 바꾼 식을 여러개 더해 합계를 더해본다.
    * X1 = T + E1, X2 = T + E2, X3 = T + E3
    * E1, E2, E3의 총합을 구하는 과정에서 E값이 줄어들기를 기대하는 것.

## 설문 만들어보기
1. 주제 정하기 : 연애 스타일
2. 설문 항목 정할 때 : 연애를 표현하는 단어나 문장 먼저 만들어보기 ex.성격 형용사(O.C.E.A.N.)
3. 문항 3~4개 만들기, 이 때 문항들은 공통 요소에 대한 것으로.

4. Feedback
    1. 한 달 간 데이트하는 횟수는? 1) 1-2회 2) 3-4회 3) 5-7회 4) 8회 이상  
        -> 최근 한 달 간 / 지난 한 달 간 등 시기를 특정해야 왜곡을 막을 수 있다.  
        -> 응답을 주관식으로 받아도 된다.  
    2. 만나면 평균적으로 데이트를 몇 시간 하는가? 1) 1시간 미만 2) 1-3시간 3) 3-5시간 4) 5시간 초과  
        -> 역시 시기 특정(직전 데이트는 몇 시간 동안 했는가?)  
    3. 만나서 주로 가는 곳은? 1) 영화관 2) 식당 3) 카페 4) 술집  
        -> 강제 선택형에 해당함, 이렇게 물어보지 말 것!  
        -> 각 항목을 개별적으로 질문. 지난 한 달 간 영화관 데이트를 몇 번 했는가?  
    4. 데이트 한 번에 비용으로 얼마나 지출하는가? 1) 2만원 미만 2) 2-5만원 3) 5-10만원 4) 10만원 초과  
        -> 시기 특정하기  
        -> 금액 같은 경우도 주관식으로 받아도 된다. 단, 응답자가 귀찮게 느낄 수 있음.  

# MAB

## 탐색과 활용
* 탐색 : 기존과 다른 시도, 새로운 지식을 알게 되는 기회가 되지만 그만큼 시행착오도 많이 겪을 수 있다.
* 활용 : 기존의 방법을 계속, 개선의 기회가 없을 수 있다.

### 슬롯 머신 예시 : 여러 팔 강도 (Multi-Armed Bandit, 각기 다른 확률을 가진 여러 대의 슬롯 머신)
* 탐색만 하는 전략
    * 모든 슬롯머신을 골고루 당겨본다.
    * 각 슬롯머신의 수익률을 가장 정확히 파악 가능
    * 많이 잃지도 않지만 많이 벌지도 못 한다. -> 모든 슬롯머신의 평균만큼만 벌 수 있음.

## 용어
* 행동(Action) : 하나의 선택 ex. 슬롯머신, 홈페이지의 디자인
* 보상(Reward) : 각 선택의 결과 ex. 발생한 수익, 고객의 선택
* 가치(Value) : 각 행동에 따르는 보상의 평균

## 종류
1. 탐욕법(Greedy Algorithm)
    * 지금 현재 가장 가치가 높은 행동을 선택 (활용 중심)
    * 일정 비율(ε, epsilon)만큼은 탐색
    * epsilon-first : 처음에 ε만큼은 탐색을 하고 그 이후는 활용
        * 예시 : 슬롯머신을 각각 100번씩 당겨보고 이후로는 가치가 가장 높은 슬롯머신만 당긴다.
        * 문제 : 충분히 탐색을 하지 못할 가능성 존재, 시간에 따라 변화하는 상황에 대응하지 못함.
    * epsilon-greedy : 매번 ε의 확률로 탐색, 그렇지 않으면 활용
        * 예시 : 주사위를 굴려서 1이 나오면 무작위로 아무 슬롯머신 작동, 그 외는 이제까지 가치가 가장 높은 슬롯머신 작동
        * 문제 : 기존 데이터가 많이 누적되면 상황이 변해도 반영이 어려움
        * 보완 방법 : 낙관적 초기화(새로운 옵션에 초기값을 많이 주어 기회를 많이 주는 것)


```python
from numpy.random import normal
from numpy.random import uniform
import random

count = {'A':0, 'B':0}
reward = {'A':0, 'B':0}
value = {'A':0, 'B':0}

# 0~1 사이의 난수 하나 생성 후 입실론보다 작으면 탐색, 아니면 활용
for _ in range(1000):
    if uniform(0, 1) < epsilon:
        if random.choice('AB') == 'A':
            reward['A'] += normal(10, 1)
            count['A'] += 1
            value['A'] = reward['A'] / count['A']
        else:
            reward['B'] += normal(9, 1)
            count['B'] += 1
            value['B'] = reward['B'] / count['B']
    else:
        if value['A']>value['B']:
            reward['A'] += normal(10, 1)
            count['A'] += 1
            value['A'] = reward['A'] / count['A']
        else:
            reward['B'] += normal(9, 1)
            count['B'] += 1
            value['B'] = reward['B'] / count['B']
```


```python
# 뭔지 잘 모르겠는 방법1,,,


real = {'A':0, 'B':0}
value = {'A':0, 'B':0}

for _ in range(10):
    if uniform(0, 1) < epsilon:
        s = random.choice('AB')
    else:
        if value['A'] > value['B']:
            s = 'A'
        else:
            s = 'B'
    
    reward[s] += normal(real[s], 1)
    count[s] += 1
    value[s] = reward[s] / 100
    print(s)
    
    if time > 500:
        real['A'] = 0
```

```python
# 뭔지 잘 모르겠는 방법2,,,


real = {'A':0, 'B':0}
value = {'A':0, 'B':0}

for _ in range(10):
    if uniform(0, 1) < epsilon:
        s = random.choice('AB')
    else:
        if value['A'] > value['B']:
            s = 'A'
        else:
            s = 'B'
    
    reward[s] += normal(real[s], 1)
    count[s] += 1
    value[s] = reward[s] / 100
    print(s)
    
    if time > 500:
        real['A'] = 0
```